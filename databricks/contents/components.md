# The Components

## Spark Core {#sparkcore}

Contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, interacting with storage systems and more. Spark core is also home to the API that defines resilient distributed datasets (RDD), which are Spark’s main programming abstraction.

RDDs represents a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections. Below are core classes:

- `pyspark.SparkContext`. Main entry point for Spark functionality.
- `pyspark.RDD`. The basic abstraction in Spark
- `pyspark.streaming.StreamingContext`. Main entry point for Spark streaming functionality
- `pyspark.streaming.Dstream`. The basic abstraction in Spark Streaming
- `pyspark.sql.SparkSession`. Main entry point for DataFrame and SQL functionality.
- `pyspark.sql.DataFrame`. A distributed collection of data grouped into columns

### Spark SQL

Spark SQL is Spark package for working with structured data. It allows querying data via SQL as well as the Apache Hive variant of SQL -called the Hive Query Language (HQL)-. It supports many sources of data, including Hive tables, **Parquet**, and JSON. Beyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java and Scala, all within single application.

### Spark Streaming

Spark Streaming is Spark component that enables processing of live streams of data. Examples of data streams include logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service.

Spark streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API, making it easy for pro-grammers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. Underneath is API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.

### `MLlib`

Spark comes with a library containing common machine learning (ML) functionality, called `Mllib`. `Mllib` provides multiple types of machine learning algorithms, including **classification**, **regression**, **clustering**, and **collaborative filtering** as well as supporting functionality such as **model evaluation** and **data import**. It also provides some *lower-level ML primitives*, including a generic gradient descent optimization algorithm.

### GraphX

GraphX is a library for manipulating graphs and performing graph-parallel computations. Like Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. GraphX also provides various operators for manipulating graphs and a library of common graph algorithms.

## Cluster Managers {#clustermanager}

Spark is designed to efficiently scale up from one to many thousands of computer nodes. To achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, including Hadoop YARN, Apache Mesos, and a simple cluster manager included in Spark itself called the *Standalone Scheduler*.

## Storage Layers {#storagelayer}

Spark can create distributed datasets from any file stored in the Ha-doop distributed filesystem (HDFS) or other storage systems supported by the Hadoop APIs (including your local filesystem, Amazon S3, Cas-sandra, Hive, Hbase, etc). It’s important to remember that **Spark does not require Hadoop**; it simply has support for storage systems imple-menting the Hadoop APIs. Spark supports text files, SequenceFiles, Avro, Parquet, and any other Hadoop InputFormat.
