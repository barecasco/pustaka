
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>The Components Â· HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 3.6.6">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="rdd_programming.html" />
    
    
    <link rel="prev" href="spark_history.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">Resume</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Cover
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="setup.html">
            
                <a href="setup.html">
            
                    
                    Setup
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="setup.html">
            
                <a href="setup.html#sparkdown">
            
                    
                    Spark Download
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="setup.html">
            
                <a href="setup.html#sparkwin">
            
                    
                    Install Spark on Windows
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="setup.html">
            
                <a href="setup.html#sparkshell">
            
                    
                    Spark Shell
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="setup.html">
            
                <a href="setup.html#sparkjup">
            
                    
                    Using Spark in jupyter
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="setup.html">
            
                <a href="setup.html#sparkalone">
            
                    
                    Standalone Spark
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="setup.html">
            
                <a href="setup.html#sparkshut">
            
                    
                    Shutdown Spark
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="header">Contents</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="what_is_spark.html">
            
                <a href="what_is_spark.html">
            
                    
                    What is Spark?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="spark_history.html">
            
                <a href="spark_history.html">
            
                    
                    History of Spark
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="2.3" data-path="components.html">
            
                <a href="components.html">
            
                    
                    The Components
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.3.1" data-path="components.html">
            
                <a href="components.html#sparkcore">
            
                    
                    Spark Core
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3.2" data-path="components.html">
            
                <a href="components.html#clustermanager">
            
                    
                    Cluster Manager
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3.3" data-path="components.html">
            
                <a href="components.html#storagelayer">
            
                    
                    Storage Layer
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="rdd_programming.html">
            
                <a href="rdd_programming.html">
            
                    
                    Programming with RDDs
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Published with HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >The Components</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
                                <section class="normal markdown-section">
                                
                                <h1 id="the-components">The Components</h1>
<h2 id="sparkcore">Spark Core </h2>
<p>Contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, interacting with storage systems and more. Spark core is also home to the API that defines resilient distributed datasets (<a href="../GLOSSARY.html#rdd" class="glossary-term" title="Represents a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.">RDD</a>), which are Spark&#x2019;s main programming abstraction.</p>
<p>RDDs represents a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections. Below are core classes:</p>
<ul>
<li><code>pyspark.SparkContext</code>. Main entry point for Spark functionality.</li>
<li><code>pyspark.RDD</code>. The basic abstraction in Spark</li>
<li><code>pyspark.streaming.StreamingContext</code>. Main entry point for Spark streaming functionality</li>
<li><code>pyspark.streaming.Dstream</code>. The basic abstraction in Spark Streaming</li>
<li><code>pyspark.sql.SparkSession</code>. Main entry point for DataFrame and SQL functionality.</li>
<li><code>pyspark.sql.DataFrame</code>. A distributed collection of data grouped into columns</li>
</ul>
<h3 id="spark-sql">Spark SQL</h3>
<p>Spark SQL is Spark package for working with structured data. It allows querying data via SQL as well as the Apache Hive variant of SQL -called the Hive Query Language (HQL)-. It supports many sources of data, including Hive tables, <strong>Parquet</strong>, and JSON. Beyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java and Scala, all within single application.</p>
<h3 id="spark-streaming">Spark Streaming</h3>
<p>Spark Streaming is Spark component that enables processing of live streams of data. Examples of data streams include logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service.</p>
<p>Spark streaming provides an API for manipulating data streams that closely matches the Spark Core&#x2019;s <a href="../GLOSSARY.html#rdd" class="glossary-term" title="Represents a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.">RDD</a> API, making it easy for pro-grammers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. Underneath is API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.</p>
<h3 id="mllib"><code>MLlib</code></h3>
<p>Spark comes with a library containing common <a href="../GLOSSARY.html#machine" class="glossary-term" title="A machine consists of a certain number of nodes,
each of which con-sists of a certain number of cores.">machine</a> learning (ML) functionality, called <code>Mllib</code>. <code>Mllib</code> provides multiple types of <a href="../GLOSSARY.html#machine" class="glossary-term" title="A machine consists of a certain number of nodes,
each of which con-sists of a certain number of cores.">machine</a> learning algorithms, including <strong>classification</strong>, <strong>regression</strong>, <strong>clustering</strong>, and <strong>collaborative filtering</strong> as well as supporting functionality such as <strong>model evaluation</strong> and <strong>data import</strong>. It also provides some <em>lower-level ML primitives</em>, including a generic gradient descent optimization algorithm.</p>
<h3 id="graphx">GraphX</h3>
<p>GraphX is a library for manipulating graphs and performing graph-parallel computations. Like Spark Streaming and Spark SQL, GraphX extends the Spark <a href="../GLOSSARY.html#rdd" class="glossary-term" title="Represents a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.">RDD</a> API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. GraphX also provides various operators for manipulating graphs and a library of common graph algorithms.</p>
<h2 id="clustermanager">Cluster Managers </h2>
<p>Spark is designed to efficiently scale up from one to many thousands of computer nodes. To achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, including <a href="../GLOSSARY.html#hadoop" class="glossary-term" title="Is often used to run ad-hoc exploratory queries on a large datasets,
through SQL interfaces such as Pig and Hive.
Ideally, a user would be able to load dataset of interest into memory accross
a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant
latency (tens of seconds) because it runs as separate MapReduce job and reads data from disk.">Hadoop</a> YARN, Apache Mesos, and a simple cluster manager included in Spark itself called the <em>Standalone Scheduler</em>.</p>
<h2 id="storagelayer">Storage Layers </h2>
<p>Spark can create distributed datasets from any file stored in the Ha-doop distributed filesystem (HDFS) or other storage systems supported by the <a href="../GLOSSARY.html#hadoop" class="glossary-term" title="Is often used to run ad-hoc exploratory queries on a large datasets,
through SQL interfaces such as Pig and Hive.
Ideally, a user would be able to load dataset of interest into memory accross
a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant
latency (tens of seconds) because it runs as separate MapReduce job and reads data from disk.">Hadoop</a> APIs (including your local filesystem, Amazon S3, Cas-sandra, Hive, Hbase, etc). It&#x2019;s important to remember that <strong>Spark does not require <a href="../GLOSSARY.html#hadoop" class="glossary-term" title="Is often used to run ad-hoc exploratory queries on a large datasets,
through SQL interfaces such as Pig and Hive.
Ideally, a user would be able to load dataset of interest into memory accross
a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant
latency (tens of seconds) because it runs as separate MapReduce job and reads data from disk.">Hadoop</a></strong>; it simply has support for storage systems imple-menting the <a href="../GLOSSARY.html#hadoop" class="glossary-term" title="Is often used to run ad-hoc exploratory queries on a large datasets,
through SQL interfaces such as Pig and Hive.
Ideally, a user would be able to load dataset of interest into memory accross
a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant
latency (tens of seconds) because it runs as separate MapReduce job and reads data from disk.">Hadoop</a> APIs. Spark supports text files, SequenceFiles, Avro, Parquet, and any other <a href="../GLOSSARY.html#hadoop" class="glossary-term" title="Is often used to run ad-hoc exploratory queries on a large datasets,
through SQL interfaces such as Pig and Hive.
Ideally, a user would be able to load dataset of interest into memory accross
a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant
latency (tens of seconds) because it runs as separate MapReduce job and reads data from disk.">Hadoop</a> InputFormat.</p>

                                
                                </section>
                            
                        </div>
                    </div>
                
            </div>

            
                
                <a href="spark_history.html" class="navigation navigation-prev " aria-label="Previous page: History of Spark">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="rdd_programming.html" class="navigation navigation-next " aria-label="Next page: Programming with RDDs">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"The Components","level":"2.3","depth":1,"next":{"title":"Programming with RDDs","level":"2.4","depth":1,"path":"contents/rdd_programming.md","ref":"contents/rdd_programming.md","articles":[]},"previous":{"title":"History of Spark","level":"2.2","depth":1,"path":"contents/spark_history.md","ref":"contents/spark_history.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["katex","-lunr","-search"],"pluginsConfig":{"katex":{},"highlight":{},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Lora","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":true},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"contents/components.md","mtime":"2020-09-20T10:13:02.193Z","type":"markdown"},"gitbook":{"version":"3.6.6","time":"2020-09-21T14:51:56.982Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

